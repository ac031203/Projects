{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67975,"databundleVersionId":7564075,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import defaultdict\nfrom time import time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-10T16:15:29.197539Z","iopub.execute_input":"2024-02-10T16:15:29.197960Z","iopub.status.idle":"2024-02-10T16:15:29.207379Z","shell.execute_reply.started":"2024-02-10T16:15:29.197930Z","shell.execute_reply":"2024-02-10T16:15:29.205948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nfrom tqdm import tqdm\nimport pandas as pd\nimport random\nimport ast\nimport math\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:15:32.743149Z","iopub.execute_input":"2024-02-10T16:15:32.743526Z","iopub.status.idle":"2024-02-10T16:15:32.751454Z","shell.execute_reply.started":"2024-02-10T16:15:32.743498Z","shell.execute_reply":"2024-02-10T16:15:32.750619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/assignment-1-nlp/train.csv') # loading training data\ndata = []\nfor index, row in tqdm(df.iterrows()):\n    data.append(ast.literal_eval(row['tagged_sentence'])) # changing data-type of entries from 'str' to 'list'","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:15:35.200845Z","iopub.execute_input":"2024-02-10T16:15:35.201233Z","iopub.status.idle":"2024-02-10T16:15:45.512839Z","shell.execute_reply.started":"2024-02-10T16:15:35.201200Z","shell.execute_reply":"2024-02-10T16:15:45.511631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/assignment-1-nlp/test_small.csv') # loading test data\ntest_data = {} \nfor index, row in tqdm(df.iterrows()):\n    test_data[row['id']] = ast.literal_eval(row['untagged_sentence']) # changing data-type of entries from 'str' to 'list'","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:15:45.514342Z","iopub.execute_input":"2024-02-10T16:15:45.514580Z","iopub.status.idle":"2024-02-10T16:15:45.999002Z","shell.execute_reply.started":"2024-02-10T16:15:45.514559Z","shell.execute_reply":"2024-02-10T16:15:45.998156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_data(sentence_index):\n    '''\n        Input : 'sentence_index' (int) -> index of a sentence in training data\n        Output: None\n    '''\n    sentence = data[sentence_index]\n    print(\"TOKEN -> TAG\")\n    print('...')\n    for token, tag in sentence:\n        print(token, '>', tag)\nsentence_index = random.choice(range(len(data)))\n#display_data(sentence_index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cell to show the frequency of each distinct (slack or native) present in the training data\nfrom collections import Counter\ndistinct_tags = []\nword_tags = []\ndef store_tags():\n    \n    global distinct_tags\n    global word_tags\n    \n    for sent in data:\n        word_tags.append(('START','START'))\n        for words, tag in sent:\n            word_tags.extend([(tag, words)])\n        word_tags.append(('END','END'))\n    \nstore_tags()\ntags=[]\ntokens = []\nfreq = {}\nposs_tags = {}\nfor tag, words in word_tags:\n    tags.append(tag)\n    tokens.append(words)\n    if freq.get(words) is None:\n        freq[words] = 1\n    else:\n        freq[words] += 1\n    if poss_tags.get(words) is None:\n        poss_tags[words] = []\n    else:\n        poss_tags[words].append(tag)\ndistinct_tags=list(set(tags))\ndistinct_tokens = list(set(tokens))\ncount_tags = {}\nfor tag, count in Counter(tags).items():\n    count_tags[tag] = count\ntag_ind = {}\ni = 0\ntag_map = {}\nfor tag in distinct_tags:\n    tag_ind[tag] = i\n    i += 1\ntoken_ind = {}\nfor i,tag in enumerate(distinct_tags):\n    tag_map[tag] = i\nfor i in tqdm(range(len(distinct_tokens))):\n    token_ind[distinct_tokens[i]] = i\nunknown = {}\nfor tags in distinct_tags:\n    unknown[tags] = 0\nfor key, value in freq.items():\n    if value < 10:\n        for tags in poss_tags[key]:\n            unknown[tags] += 1\ntotal_count = sum(unknown.values())\n\nsmoothing_factor = 0.0001\n\n# Calculate the total number of unique elements\ntotal_unique_elements = len(unknown)\n\n# Calculate the smoothed probabilities\nsmoothed_probs = {}\nfor key, value in unknown.items():\n    smoothed_probs[key] = (value + smoothing_factor) / (total_count + smoothing_factor * total_unique_elements)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:15:51.419006Z","iopub.execute_input":"2024-02-10T16:15:51.419352Z","iopub.status.idle":"2024-02-10T16:15:53.347504Z","shell.execute_reply.started":"2024-02-10T16:15:51.419325Z","shell.execute_reply":"2024-02-10T16:15:53.346030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12, 6))\nplt.xticks(rotation='vertical')\nplt.bar(range(len(count_tags)), list(count_tags.values()), align='center')\nplt.xticks(range(len(count_tags)), list(count_tags.keys()))\nplt.xlabel('Tag')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T13:08:21.482556Z","iopub.execute_input":"2024-02-10T13:08:21.483086Z","iopub.status.idle":"2024-02-10T13:08:22.068925Z","shell.execute_reply.started":"2024-02-10T13:08:21.483045Z","shell.execute_reply":"2024-02-10T13:08:22.067478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = {'id': [], 'tagged_sentence' : []} # dictionary to store tag predictions\n# NOTE ---> ensure that tagged_sentence's corresponing 'id' is same as 'id' of corresponding 'untagged_sentence' in training data\ndef store_submission(sent_id, tagged_sentence):\n    \n    global submission\n    submission['id'].append(sent_id)\n    submission['tagged_sentence'].append(tagged_sentence)\n    \ndef clear_submission():\n    global submission\n    submission = {'id': [], 'tagged_sentence' : []}","metadata":{"execution":{"iopub.status.busy":"2024-02-10T09:50:58.451656Z","iopub.execute_input":"2024-02-10T09:50:58.452115Z","iopub.status.idle":"2024-02-10T09:50:58.465413Z","shell.execute_reply.started":"2024-02-10T09:50:58.452081Z","shell.execute_reply":"2024-02-10T09:50:58.463203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"T = len(distinct_tags) #number of states\nV = len(distinct_tokens) #vocabulary size","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:15:58.912857Z","iopub.execute_input":"2024-02-10T16:15:58.913223Z","iopub.status.idle":"2024-02-10T16:15:58.918560Z","shell.execute_reply.started":"2024-02-10T16:15:58.913193Z","shell.execute_reply":"2024-02-10T16:15:58.917166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating Initial A, B and Pi Matrices","metadata":{}},{"cell_type":"code","source":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n\ndef t2_given_t1(t2, t1, tags): # A_t1,t2 \n    count_t1 = np.sum(tags == t1)\n    count_t2_t1 = np.sum((tags[:-1] == t1) & (tags[1:] == t2))\n    return count_t2_t1,count_t1\n\ntags = np.array([pair[0] for pair in word_tags])\n\nA = np.zeros((T, T), dtype='float32')\nfor i, t1 in enumerate(list(distinct_tags)):\n    for j, t2 in enumerate(list(distinct_tags)): \n        count_t2_t1, count_t1 = t2_given_t1(t2, t1, tags)\n        A[i, j] = (0.001+count_t2_t1)/ (0.001*T+count_t1)\n        \n\n\n#tags_df = pd.DataFrame(A, columns=list(distinct_tags), index=list(distinct_tags))\n#tags_df","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:16:01.913324Z","iopub.execute_input":"2024-02-10T16:16:01.913679Z","iopub.status.idle":"2024-02-10T16:16:35.944570Z","shell.execute_reply.started":"2024-02-10T16:16:01.913653Z","shell.execute_reply":"2024-02-10T16:16:35.943340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_given_tag(word, tag, tag_word_counts): #emission probability b_tag(word)\n    count_tag = count_tags[tag]\n    count_w_given_tag = tag_word_counts.get(tag, {}).get(word, 0)\n    \n#     if(count_w_given_tag == 0):\n#         return (0.001+count_tag)/(0.001*V+len(tags))\n    return (0.00001+count_w_given_tag)/ (0.00001*V+count_tag)\n\n# Preprocess train_bag to create a dictionary of counts for each (tag, word) combination\ntag_word_counts = {}\nfor pair_tag, word_token in word_tags:\n    tag_word_counts.setdefault(pair_tag, {}).setdefault(word_token, 0)\n    tag_word_counts[pair_tag][word_token] += 1\n\n# Calculate emission probabilities\nB = np.zeros((T, V), dtype='float32')\nfor i, t1 in enumerate(list(distinct_tags)):\n    for j, t2 in enumerate(list(distinct_tokens)):\n        B[i, j] = word_given_tag(t2, t1, tag_word_counts)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:16:46.961907Z","iopub.execute_input":"2024-02-10T16:16:46.962282Z","iopub.status.idle":"2024-02-10T16:16:50.036275Z","shell.execute_reply.started":"2024-02-10T16:16:46.962253Z","shell.execute_reply":"2024-02-10T16:16:50.035417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compute pi -> initial tag probability\npi_vector = np.zeros(T, dtype = 'float32')\nfor i in range(T):\n    pi_vector[i] = A[tag_ind[\"START\"],i]\n# pi_df = pd.DataFrame(pi_vector, index = list(distinct_tags))\n# pi_df\n#pi_vector","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:16:52.830332Z","iopub.execute_input":"2024-02-10T16:16:52.831441Z","iopub.status.idle":"2024-02-10T16:16:52.837223Z","shell.execute_reply.started":"2024-02-10T16:16:52.831406Z","shell.execute_reply":"2024-02-10T16:16:52.835901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compute phi -> end tag probability\nphi_vector = np.zeros(T, dtype = 'float32')\nfor i in range(T):\n    phi_vector[i] = A[i,tag_ind[\"END\"]]\n\n#phi_vector","metadata":{"execution":{"iopub.status.busy":"2024-02-10T12:22:03.168512Z","iopub.execute_input":"2024-02-10T12:22:03.169161Z","iopub.status.idle":"2024-02-10T12:22:03.174874Z","shell.execute_reply.started":"2024-02-10T12:22:03.169125Z","shell.execute_reply":"2024-02-10T12:22:03.173450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EM Algorithm implementation using log- sum trick","metadata":{}},{"cell_type":"code","source":"def log_sum_exp(values):\n    max_value = np.max(values)\n    return max_value + np.log(np.sum(np.exp(values - max_value)))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T12:22:06.094558Z","iopub.execute_input":"2024-02-10T12:22:06.095063Z","iopub.status.idle":"2024-02-10T12:22:06.101412Z","shell.execute_reply.started":"2024-02-10T12:22:06.095025Z","shell.execute_reply":"2024-02-10T12:22:06.099946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forward_prob(sentence):\n    T = len(sentence)\n    N = len(distinct_tags)\n    alpha = np.zeros((N, T), dtype='float32')\n    \n    # Initialize alpha matrix for the first word\n    if sentence[0] not in token_ind:\n        alpha[:, 0] = np.log(pi_vector) + np.log(smoothed_probs[distinct_tags])\n    else:\n        alpha[:, 0] = np.log(pi_vector) + np.log(B[:, token_ind[sentence[0]]])\n    \n    # Recursively fill alpha matrix using log-sum-exp trick\n    for t in range(1, T):\n        if sentence[t] not in token_ind:\n            emission_prob = np.log(smoothed_probs[distinct_tags])\n        else:\n            emission_prob = np.log(B[:, token_ind[sentence[t]]])\n        alpha[:, t] = log_sum_exp(alpha[:, t-1] + np.log(A.T)) + emission_prob\n    \n    end_vector = alpha[:,-1] \n    end_val = log_sum_exp(end_vector)\n    \n    return np.exp(alpha), np.exp(end_val)\n\n#forward_prob([\"This\",\"is\",\"test\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-10T13:10:10.672527Z","iopub.execute_input":"2024-02-10T13:10:10.672995Z","iopub.status.idle":"2024-02-10T13:10:10.685207Z","shell.execute_reply.started":"2024-02-10T13:10:10.672959Z","shell.execute_reply":"2024-02-10T13:10:10.684020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def backward_prob(sentence):\n    T = len(sentence)\n    N = len(distinct_tags)\n    beta = np.zeros((N, T), dtype='float32')\n    \n    # Initialize beta matrix for the last word\n    beta[:, -1] = 0\n    \n    # Recursively fill beta matrix using log probabilities and log-sum-exp trick\n    for t in range(T-2, -1, -1):\n        for s in range(N):\n            if sentence[t+1] not in token_ind:\n                cur = log_sum_exp(beta[:, t+1] + np.log(A[s, :]) + np.log(smoothed_probs[distinct_tags]))\n            else:\n                cur = log_sum_exp(beta[:, t+1] + np.log(A[s, :]) + np.log(B[:, token_ind[sentence[t+1]]]))\n            beta[s, t] = cur\n   \n    if sentence[0] not in token_ind:\n        start_state = beta[:, 0] + np.log(smoothed_probs[distinct_tags]) + np.log(pi_vector)\n    else:\n        start_state = beta[:, 0] + np.log(B[:, token_ind[sentence[0]]]) + np.log(pi_vector)\n    start_state_val = log_sum_exp(start_state)\n    \n    return np.exp(beta), np.exp(start_state_val)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T13:10:08.698304Z","iopub.execute_input":"2024-02-10T13:10:08.699655Z","iopub.status.idle":"2024-02-10T13:10:08.711996Z","shell.execute_reply.started":"2024-02-10T13:10:08.699604Z","shell.execute_reply":"2024-02-10T13:10:08.710425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xi_prob(sentence, forward, backward, forward_val):\n    T = len(sentence)\n    N = len(distinct_tags)\n    xi_log_probabilities = np.zeros((N, T-1, N), dtype='float32')\n    \n    for t in range(T-1):\n        for i in range(N):\n            if sentence[t+1] not in token_ind:\n                emission_log_prob = np.log(smoothed_probs[distinct_tags])\n            else:\n                emission_log_prob = np.log(B[:, token_ind[sentence[t+1]]])\n            xi_log_probabilities[i, t, :] = forward[i, t] + backward[:, t+1] + np.log(A[i, :]) + emission_log_prob - forward_val\n    \n    # Convert log probabilities to regular probabilities\n    xi_probabilities = np.exp(xi_log_probabilities)\n    \n    return xi_probabilities","metadata":{"execution":{"iopub.status.busy":"2024-02-10T13:10:21.216815Z","iopub.execute_input":"2024-02-10T13:10:21.217283Z","iopub.status.idle":"2024-02-10T13:10:21.227859Z","shell.execute_reply.started":"2024-02-10T13:10:21.217251Z","shell.execute_reply":"2024-02-10T13:10:21.226438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gamma_prob(sentence, forward, backward, forward_val):\n    N = len(distinct_tags)\n    gamma_log_probabilities = forward + backward - forward_val\n    \n    # Convert log probabilities to regular probabilities\n    gamma_probabilities = np.exp(gamma_log_probabilities)\n\n    # Normalize gamma probabilities\n    row_sums = np.sum(gamma_probabilities, axis=1, keepdims=True)\n    gamma_probabilities /= row_sums\n\n    return gamma_probabilities","metadata":{"execution":{"iopub.status.busy":"2024-02-10T13:10:23.177047Z","iopub.execute_input":"2024-02-10T13:10:23.177504Z","iopub.status.idle":"2024-02-10T13:10:23.185224Z","shell.execute_reply.started":"2024-02-10T13:10:23.177472Z","shell.execute_reply":"2024-02-10T13:10:23.183907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **HMM Learning**","metadata":{}},{"cell_type":"code","source":"for iteration in tqdm(range(100)):\n    sentence = data[iteration]\n    sent = [token for token, tag in sentence]\n    T = len(sentence)\n    N = len(distinct_tags)\n\n    fwd_probs, fwd_val = forward_prob(sent)\n    bwd_probs, bwd_val = backward_prob(sent)\n    xi_probabilities = xi_prob(sent, fwd_probs, bwd_probs, fwd_val)\n    gamma_probabilities = gamma_prob(sent, fwd_probs, bwd_probs, fwd_val)\n\n    a = np.zeros((N, N))\n    \n    # Initialize emission matrix 'b' with correct dimensions\n    b = np.zeros((N, V))\n\n    # Update 'A' matrix using xi probabilities\n    for t in range(T - 1):\n        for i in range(N):\n            for j in range(N):\n                a[i, j] += xi_probabilities[i, t, j]\n    \n    # Normalize 'A' matrix\n    a /= np.sum(a, axis=1, keepdims=True)\n    \n    # Update 'B' matrix using gamma probabilities\n    for j in range(N):\n        for vk in range(T):\n            # Ensure that the index is within the valid range\n            if vk < V:\n                indices = np.where(np.array(sent) == sent[vk])[0]\n                numerator_b = np.sum(gamma_probabilities[j, indices])\n                denominator_b = np.sum(gamma_probabilities[j, :])\n\n                if denominator_b != 0:\n                    b[j, vk] = numerator_b / denominator_b\n\n    b[np.isnan(b)] = 0.001\n\n    transition = a\n    emission = b\n\n#     new_fwd_temp, new_fwd_temp_val = forward_prob(sent)\n#     print('New forward probability: ', new_fwd_temp_val)\n#     diff = np.abs(fwd_val - new_fwd_temp_val)\n#     print('Difference in forward probability: ', diff)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T13:10:46.824065Z","iopub.execute_input":"2024-02-10T13:10:46.824879Z","iopub.status.idle":"2024-02-10T13:11:05.549392Z","shell.execute_reply.started":"2024-02-10T13:10:46.824830Z","shell.execute_reply":"2024-02-10T13:11:05.548180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Rule based tagging for unknown words**","metadata":{}},{"cell_type":"code","source":"endings_to_tags = {\n    \"ing\": \"VB\", \"ed\": \"VB\", \"es\": \"VB\", \"ly\": \"RB\",\n    \"s\": \"NN\", \"ness\": \"NN\", \"ment\": \"NN\", \"ful\": \"JJ\",\n    \"ive\": \"JJ\", \"able\": \"JJ\", \"al\": \"JJ\", \"ic\": \"JJ\",\n    \"er\": \"NN\", \"or\": \"NN\", \"ion\": \"NN\", \"ity\": \"NN\"\n}\n\ndef assign_pos_tag(word):\n    if word.isdigit():\n        return \"CD\"  # Tag words involving a digit as numbers\n    elif word.istitle():\n        return \"NN\"  # Tag capitalized words as singular proper nouns\n    else:\n        # Check if the word ends with any of the defined endings\n        for ending, tag in endings_to_tags.items():\n            if word.endswith(ending):\n                return tag\n        # If none of the above conditions match, label as singular or mass noun\n        return \"NN\"","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:17:02.429608Z","iopub.execute_input":"2024-02-10T16:17:02.429950Z","iopub.status.idle":"2024-02-10T16:17:02.437014Z","shell.execute_reply.started":"2024-02-10T16:17:02.429924Z","shell.execute_reply":"2024-02-10T16:17:02.436067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Viterbi Algorithm using DP**","metadata":{}},{"cell_type":"code","source":"def Viterbi(words, A, B):\n    T = len(words)\n    N = len(distinct_tags)\n    \n    viterbi = np.zeros((N, T), dtype='float32')\n    backpointer = np.zeros((N, T), dtype=int)\n    \n    # Initialize viterbi matrix for the first word\n    for s in range(N):\n        if words[0] not in token_ind:\n            tag = assign_pos_tag(words[0])\n            if(distinct_tags[s]!=tag):\n                viterbi[s][0] = math.log(pi_vector[s]) + math.log(0.0001)\n            else:\n                viterbi[s][0] = math.log(pi_vector[s]) + math.log(0.9999)\n        else:\n            emission_prob = math.log(B[s, token_ind[words[0]]]) if B[s, token_ind[words[0]]] != 0 else float('-inf')\n            viterbi[s][0] = math.log(pi_vector[s]) + emission_prob\n            \n        backpointer[s][0] = -1\n    \n    # Recursively fill viterbi matrix\n    for t in range(1, T):\n        for s in range(N):\n            if words[t] not in token_ind:\n                tag = assign_pos_tag(words[t])\n                emission_prob = math.log(0.9999) if distinct_tags[s] == tag else math.log(0.0001)\n            else:\n                emission_prob = math.log(B[s, token_ind[words[t]]]) if B[s, token_ind[words[t]]] != 0 else float('-inf')\n            \n            probabilities = viterbi[:, t-1] + np.log(A[:, s]) + emission_prob\n            bestpath = np.argmax(probabilities)\n            \n            backpointer[s][t] = bestpath\n            viterbi[s][t] = probabilities[bestpath]\n    \n    # Backtrack to find the best tag sequence\n    bestpathpointer = np.argmax(viterbi[:, T-1])\n    tags = []\n    for t in range(T-1, -1, -1):\n        if(words[t] not in token_ind):\n            tags.append((words[t],assign_pos_tag(words[t])))\n        else:\n            tags.append((words[t], distinct_tags[bestpathpointer]))\n        bestpathpointer = backpointer[bestpathpointer][t]\n    \n    return tags[::-1]","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:17:05.605300Z","iopub.execute_input":"2024-02-10T16:17:05.605677Z","iopub.status.idle":"2024-02-10T16:17:05.619647Z","shell.execute_reply.started":"2024-02-10T16:17:05.605646Z","shell.execute_reply":"2024-02-10T16:17:05.618501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Internal function for calculating average accuracy on the training data itself\ndef calculate_accuracy(predicted_tags, expected_tags):\n    l = len(predicted_tags)\n    miss = []\n    correct_tags = 0\n    for i in range(l):\n        if(predicted_tags[i][1]==expected_tags[i][1]):\n            correct_tags += 1\n        else:\n            miss.append((predicted_tags[i][0],predicted_tags[i][1],expected_tags[i][1]))\n    correct_tags = sum(1 for pred, exp in zip(predicted_tags, expected_tags) if pred[1] == exp[1])\n    #total_tags = len(predicted_tags)\n    accuracy = (correct_tags / l) * 100\n    return accuracy , miss\ntotal_sum = 0\nfor i in tqdm(range(0, 100)):\n    sentence = data[i]\n    untagged_sentence = []\n    expected = []\n    for token, tag in sentence:\n        untagged_sentence.append(token)\n        expected.append((token,tag))\n#     print(\"My output: \",Viterbi(untagged_sentence))\n#     print(\"Expected output: \",expected)\n    acc, miss = calculate_accuracy(Viterbi(untagged_sentence,A,B),expected)\n    total_sum += acc\nprint(\"Average accuracy: \",total_sum/100)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:17:08.883062Z","iopub.execute_input":"2024-02-10T16:17:08.883403Z","iopub.status.idle":"2024-02-10T16:17:10.045018Z","shell.execute_reply.started":"2024-02-10T16:17:08.883377Z","shell.execute_reply":"2024-02-10T16:17:10.044085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hmm_tagger_util(sent_id, untagged_sentence):\n    if(sent_id in list(submission['id'])):\n        return\n    tagged_sentence = Viterbi(untagged_sentence,A,B)\n    store_submission(sent_id, tagged_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MEMM Classifier","metadata":{}},{"cell_type":"code","source":"class MEMM:\n    \n    def __init__(self,tag_map,token_ind,pi_vector):\n        self.data = data\n        self.tag_map = tag_map\n        self.token_ind = token_ind\n        self.N = len(tag_map)\n        self.V = len(token_ind)\n        self.alpha = 0.01\n        self.logP = np.empty((1,1))\n        self.pi_vector = pi_vector\n        \n    def construct_features(self):\n        word_features = np.zeros((self.N , self.V))\n        \n        for sent in self.data:\n            prev_tag = 'START'\n            for i in range(len(sent)):\n                word_features[self.tag_map[prev_tag], self.token_ind[sent[i][0]]] += 1\n        \n        return word_features\n                \n      \n    def train(self):\n        word_features = self.construct_features()\n        w = np.random.rand(self.N-2, self.N)\n    \n        for epoch in range(5):\n            log_p = np.zeros((self.N-2, self.V))\n\n            for word_ind in range(self.V):\n                denom = 0\n                for state_ind in range(self.N-2):\n                    log_p[state_ind, word_ind] = np.log(np.exp(w[state_ind, :] * word_features[:, word_ind]))\n                    denom += np.exp(w[state_ind, :] * word_features[:, word_ind])\n                log_p[:, word_ind] -= np.log(denom)\n\n            grad = np.zeros((self.N-2, self.N))\n\n            for word in range(self.V):\n                for i in range(self.N-2):\n                    for j in range(self.N):\n                        grad[i, j] += -(1-np.exp(log_p[i, word])) * word_features[j, word]\n\n            w = w - self.alpha* grad\n        \n        self.logP = np.zeros((self.N-2, self.V))\n        \n        for word_ind in range(self.V):\n            denom = 0\n            for state_ind in range(self.N-2):\n                self.logP[state_ind, word_ind] = np.log(np.exp(w[state_ind, :] * word_features[:, word_ind]))\n                denom += np.exp(w[state_ind, :] * word_features[:, word_ind])\n            self.logP[:, word_ind] -= np.log(denom)\n    \n    def predict(self , words, distinct_tags):\n        T = len(words)\n        N = self.N-2\n\n        viterbi = np.zeros((N, T), dtype='float32')\n        backpointer = np.zeros((N, T), dtype=int)\n\n        # Initialize viterbi matrix for the first word\n        for s in range(N):\n            if words[0] not in self.token_ind:\n                tag = assign_pos_tag(words[0])\n                if(distinct_tags[s]!=tag):\n                    viterbi[s][0] = math.log(0.0001)\n                else:\n                    viterbi[s][0] = math.log(0.9999)\n            else:\n                emission_prob = self.logP[s, self.token_ind[words[0]]] if self.logP[s, self.token_ind[words[0]]] != 0 else float('-inf')\n                viterbi[s][0] = emission_prob\n\n            backpointer[s][0] = -1\n\n        # Recursively fill viterbi matrix\n        for t in range(1, T):\n            for s in range(N):\n                if words[t] not in self.token_ind:\n                    tag = assign_pos_tag(words[t])\n                    emission_prob = math.log(0.9999) if distinct_tags[s] == tag else math.log(0.0001)\n                else:\n                    emission_prob = math.logP[s, self.token_ind[words[t]]] if self.logP[s, self.token_ind[words[t]]] != 0 else float('-inf')\n\n                probabilities = viterbi[:, t-1] + emission_prob\n                bestpath = np.argmax(probabilities)\n\n                backpointer[s][t] = bestpath\n                viterbi[s][t] = probabilities[bestpath]\n\n        # Backtrack to find the best tag sequence\n        bestpathpointer = np.argmax(viterbi[:, T-1])\n        tags = []\n        for t in range(T-1, -1, -1):\n            if(words[t] not in self.token_ind):\n                tags.append((words[t],assign_pos_tag(words[t])))\n            else:\n                tags.append((words[t], distinct_tags[bestpathpointer]))\n            bestpathpointer = backpointer[bestpathpointer][t]\n\n        return tags[::-1]","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:23:35.330910Z","iopub.execute_input":"2024-02-10T16:23:35.331258Z","iopub.status.idle":"2024-02-10T16:23:35.355240Z","shell.execute_reply.started":"2024-02-10T16:23:35.331230Z","shell.execute_reply":"2024-02-10T16:23:35.354040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def memm_tagger_util(sent_id, untagged_sentence):\n    if(sent_id in list(submission['id'])):\n        return\n    MEMMClassifier = MEMM(tag_map,token_ind,pi_vector)\n    tagged_sentence = MEMMClassifier.predict(pi_vector)\n    store_submission(sent_id, tagged_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cell to implement tagger that allots random tags to words in a sentence\n\ndef random_tagger_util(sent_id, untagged_sentence):\n    if(sent_id in list(submission['id'])):\n        return\n    tagged_sentence = []\n    for word in untagged_sentence:\n        tagged_sentence.append((word, random.choice(distinct_tags)))\n    store_submission(sent_id, tagged_sentence)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_id in tqdm(list(test_data.keys())):\n    sent = test_data[sent_id]\n    hmm_tagger_util(sent_id, sent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_directory = '/kaggle/working/'\npd.DataFrame(submission).to_csv(path_to_directory +' sample_submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}